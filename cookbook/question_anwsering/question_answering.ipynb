{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5151afed",
   "metadata": {},
   "source": [
    "# 问答\n",
    "\n",
    "## 用例\n",
    "此处展示了如何使用 Langchian + 千帆 SDK 完成对特定文档完成获取、切分、转为向量并存储，而后根据你的提问来从文中获取答案。\n",
    "并且借助 Langsmith 将整个过程可视化展现\n",
    "\n",
    "## 概览\n",
    "把一个非结构化的文档转成问答链涉及以下步骤：\n",
    "1. `Loading`: 首先我们需要加载数据，非结构化的数据可以从多种渠道加载。点击 [LangChain integration hub](https://integrations.langchain.com/) 查看所有 Langchain 支持的 Loader。\n",
    "每个 Loader 都会返回 Langchian 中的 [`Document`](/docs/components/schema/document) 对象。\n",
    "\n",
    "2. `Splitting`: [文本切分器](/docs/modules/data_connection/document_transformers/) 把 `Documents` 切分成特定的大小。\n",
    "\n",
    "3. `Storage`: `Storage` （例如 [vectorstore](/docs/modules/data_connection/vectorstores/)）会将切分的数据储存起来，通常还附带对文本做 [embedding](https://www.pinecone.io/learn/vector-embeddings/) 。\n",
    "\n",
    "4. `Retrieval`: 用于从 `Storage` 中获取切分的数据，用于后面生成答案。\n",
    "\n",
    "5. `Generation`: 使用提示词和获取到的数据，搭配 [LLM](/docs/modules/model_io/models/llms/) 来生成回答。\n",
    "\n",
    "6. `Conversation` (扩展): 添加 [Memory](/docs/modules/memory/) 模块来在你的问答链上实现多轮对话。\n",
    "\n",
    "![flow.jpeg](img/qa_flow.jpeg)\n",
    "\n",
    "接下来我们会演示如何一步步构造我们自己的流水线，并且实现我们自己定制化的功能\n",
    "\n",
    "## Step 0. Prepare\n",
    "\n",
    "为了能够运行我们的 Demo，首先我们需要下载依赖并且设置环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71be487-126d-4424-a568-93d3f2f3f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install qianfan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981930ef-a0b4-46f9-b60b-a495117ea38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['QIANFAN_AK'] = \"your_api_key\"\n",
    "os.environ['QIANFAN_SK'] = \"your_secret_key\"\n",
    "\n",
    "# 此处为 Langsmith 相关功能开关。当且仅当你知道这是做什么用时，可删除注释并设置变量以使用 Langsmith 相关功能\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "# os.environ['LANGCHAIN_API_KEY'] = \"your_langchian_api_key\"\n",
    "# os.environ['LANGCHAIN_PROJECT'] = \"your_project_name\"\n",
    "\n",
    "\n",
    "is_chinese = True\n",
    "\n",
    "if is_chinese:\n",
    "    WEB_URL = \"https://zhuanlan.zhihu.com/p/85289282\"\n",
    "    CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "        使用下面的语料来回答本模板最末尾的问题。如果你不知道问题的答案，直接回答 \"我不知道\"，禁止随意编造答案。\n",
    "        为了保证答案尽可能简洁，你的回答必须不超过三句话。\n",
    "        请注意！在每次回答结束之后，你都必须接上 \"感谢你的提问\" 作为结束语\n",
    "        以下是一对问题和答案的样例：\n",
    "            请问：秦始皇的原名是什么\n",
    "            秦始皇原名嬴政。感谢你的提问。\n",
    "        \n",
    "        以下是语料：\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        请问：{question}\n",
    "    \"\"\"\n",
    "    QUESTION1 = \"明朝的开国皇帝是谁\"\n",
    "    QUESTION2 = \"朱元璋是什么时候建立的明朝\"\n",
    "else:\n",
    "    WEB_URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "    CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "        Use three sentences maximum and keep the answer as concise as possible. \n",
    "        Always say \"thanks for asking!\" at the end of the answer. \n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Helpful Answer:\n",
    "    \"\"\"\n",
    "    QUESTION1 = \"How do agents use Task decomposition?\"\n",
    "    QUESTION2 = \"What are the various ways to implemet memory to support it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5daed6",
   "metadata": {},
   "source": [
    "## Step 1. Load\n",
    "\n",
    "指定一个 `DocumentLoader` 来把你指定的非结构化数据加载成 `Documents`。一个 `Document` 是文字（即 `page_content`）和与之相关的元数据的结合体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4d5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(WEB_URL)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cc9a7",
   "metadata": {},
   "source": [
    "## Step 2. Split\n",
    "\n",
    "接下来把 `Document` 切分成块，为后续的 embedding 和存入向量数据库做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b11c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 384, chunk_overlap = 0, separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"。\", \"，\"])\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33bd4d",
   "metadata": {},
   "source": [
    "## Step 3. Store\n",
    "\n",
    "为了能够查询文档的片段，我们首先需要把它们存储起来，一种比较常见的做法是对文档的内容做 embedding，然后再将 embedding 的向量连同文档一起存入向量数据库中，此处 embedding 用于索引文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c302c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [09-15 20:50:50] logging.py:55 [t:8485264192]: trying to refresh access_token\n",
      "[INFO] [09-15 20:50:50] logging.py:55 [t:8485264192]: sucessfully refresh access_token\n",
      "[INFO] [09-15 20:50:50] logging.py:55 [t:8485264192]: requesting llm api endpoint: /embeddings/embedding-v1\n",
      "[INFO] [09-15 20:50:51] logging.py:55 [t:8485264192]: requesting llm api endpoint: /embeddings/embedding-v1\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import QianfanEmbeddingsEndpoint\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=QianfanEmbeddingsEndpoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f22b0",
   "metadata": {},
   "source": [
    "除了非结构化的文档以外，Langchain 还可以从多种数据源获取数据并将它们存储起来。\n",
    "\n",
    "![lc.png](img/qa_data_load.png)\n",
    "\n",
    "## Step 4. Retrieve\n",
    "\n",
    "我们可以使用 [相似度搜索](https://www.pinecone.io/learn/what-is-similarity-search/) 来从切分的文档内获取数据，获取到的数据会作为最终提交给 LLM 的 prompt 的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c26b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search_with_relevance_scores(QUESTION1)\n",
    "[(document.page_content, score) for document, score in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d6824",
   "metadata": {},
   "source": [
    "## Step 5. Generate\n",
    "\n",
    "接下来我们就可以使用我们的大模型（例如文心一言）和 Langchain 的 RetrievalQA 链，来针对这篇文档进行提问并获取我们想要的回答了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import QianfanChatEndpoint\n",
    "\n",
    "llm = QianfanChatEndpoint()\n",
    "retriever=vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.0})\n",
    "                                   \n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
    "qa_chain({\"query\": QUESTION1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d52c84",
   "metadata": {},
   "source": [
    "注意，此处不光可以传入 `ChatModel` ，也可以传入一个 `LLM` 对象到 `RetrievalQA` 中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82f437",
   "metadata": {},
   "source": [
    "#### 自定义 prompt\n",
    "\n",
    "`RetrievalQA` 链中使用的 `prompt` 参数也是可以定制的。由于 `RetrievalQA` 链中默认提供的 prompt 是用英语编写的，此处我们可以替换为我们手动实现的中文 prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fee704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(CUSTOM_PROMPT_TEMPLATE)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "result = qa_chain({\"query\": QUESTION1})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825e9bf-6a56-46e4-8bbb-05441f76cb96",
   "metadata": {},
   "source": [
    "我们还可以选择把数据存储到千帆 Smith 的 Prompt Hub 上。需要搭配使用使用千帆 Smith SDK。\n",
    "\n",
    "一个例子如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896060f-ebc4-4236-a4ad-32960601c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qianfansmithhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8e734-ba54-48ae-b959-1898618f2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "# from qianfansmithhub import hub\n",
    "# QA_CHAIN_PROMPT_HUB = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm,\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "#     chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT_HUB}\n",
    "# )\n",
    "# result = qa_chain({\"query\": question})\n",
    "# result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40e8db",
   "metadata": {},
   "source": [
    "#### 返回源文档\n",
    "\n",
    "用于 QA 的知识文档也可以通过指定 `return_source_documents=True` 被包含在返回的字典里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60004293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)\n",
    "result = qa_chain({\"query\": QUESTION1})\n",
    "len(result['source_documents'])\n",
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b600236",
   "metadata": {},
   "source": [
    "#### 返回引用\n",
    "\n",
    "或者使用 `RetrievalQAWithSourcesChain` 来构造 Chain，以支持在返回的结果内包含结果的引用源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm,retriever=retriever)\n",
    "qa_chain({\"question\": QUESTION1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380e478-e8ae-404b-9577-6b15475a6562",
   "metadata": {},
   "source": [
    "## Step 6. Chat\n",
    "\n",
    "我们还可以加入 `Memory` 模块并替换使用 `ConversationalRetrievalChain` 来实现记忆化的对话式查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56838d-29a5-405f-a6ba-7b687ee56268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
    "qa(QUESTION1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9d803-c183-49c8-97b0-1d5fed7ac52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(QUESTION2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
